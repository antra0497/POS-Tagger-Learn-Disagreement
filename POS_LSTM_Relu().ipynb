{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required libraries"
      ],
      "metadata": {
        "id": "nJFPfo3V821-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEvYgCSyMRaz",
        "outputId": "1088dc11-7e14-442f-c2af-2f842820f36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7YyUbWTIatX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff39a143-39b2-45a2-e73b-c107d580baef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import csv\n",
        "import torch.utils.data as data_utils\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "from scipy.special import softmax\n",
        "\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "nltk.download('universal_tagset')\n",
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Importing and Data Preparation"
      ],
      "metadata": {
        "id": "vi01Lj6g9IWl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwgFgYLoKXWH"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/MscThesis/public_data_PP/gimpel_pos'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reading in the data**"
      ],
      "metadata": {
        "id": "s5hM7B9Z9lkV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRpWocMXKXdc"
      },
      "outputs": [],
      "source": [
        "def read_csv_data(path):\n",
        "  '''\n",
        "  User define function to read the .csv file from the given path\n",
        "  '''\n",
        "  data = []\n",
        "  with open(path, 'r') as f:\n",
        "    tsvreader = csv.reader(f, delimiter=\"\\t\")\n",
        "    for line in tsvreader:\n",
        "      data.append(line)\n",
        "    # to remove the header\n",
        "  return data[1:]\n",
        "\n",
        "\n",
        "def readbin(f_in):\n",
        "  '''\n",
        "  User define function to read the .bin file from the given path\n",
        "  '''\n",
        "  inp = open(f_in, \"rb\")\n",
        "  out = pickle.load(inp)\n",
        "  inp.close()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GU_seH2KXgW"
      },
      "outputs": [],
      "source": [
        "# prepared word embeddings for the model.\n",
        "word_emb_matrix = DATA_PATH + '/embeddings/word_emb_matrix.bin'\n",
        "word_emb = readbin(word_emb_matrix)\n",
        "\n",
        "\n",
        "# prepared character embeddings for the model\n",
        "char_emb_matrix = DATA_PATH + '/embeddings/char_emb_matrix.bin'\n",
        "char_emb = readbin(char_emb_matrix)\n",
        "\n",
        "\n",
        "# the train and dev data from csv\n",
        "train_path = DATA_PATH + '/input_data/train_data.tsv'\n",
        "train_data = read_csv_data(train_path)\n",
        "\n",
        "\n",
        "dev_path = DATA_PATH + '/input_data/dev_data.tsv'\n",
        "dev_data = read_csv_data(dev_path)\n",
        "\n",
        "\n",
        "# the indices of the words in a sentence, saved as arrays. Hint:Helps you know where each sentence ends\n",
        "train_iis_path = DATA_PATH + '/input_data/word_iis_trn.npy'\n",
        "dev_iis_path = DATA_PATH + '/input_data/word_iis_dev.npy'\n",
        "\n",
        "word_iis_trn = np.load(train_iis_path)\n",
        "word_iis_dev = np.load(dev_iis_path)\n",
        "\n",
        "\n",
        "# the padded sentences (maximum of 40 words per sentence). Two words in the same sentence will have the same word_pad. The numbers\n",
        "# indicate the idx of the word in the word embedding dictionary.\n",
        "train_wordpad_path = DATA_PATH + '/input_data/word_pad_trn.npy'\n",
        "dev_wordpad_path = DATA_PATH + '/input_data/word_pad_dev.npy'\n",
        "\n",
        "word_pad_trn = np.load(train_wordpad_path)\n",
        "word_pad_dev = np.load(dev_wordpad_path)\n",
        "\n",
        "# the character padding\n",
        "train_charpad_path = DATA_PATH + '/input_data/char_pad_trn.npy'\n",
        "dev_charpad_path = DATA_PATH + '/input_data/char_pad_dev.npy'\n",
        "\n",
        "char_pad_trn = np.load(train_charpad_path)\n",
        "char_pad_dev = np.load(dev_charpad_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Getting the training labels**"
      ],
      "metadata": {
        "id": "yRkC_UV5-UDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctDotC_mKXl9"
      },
      "outputs": [],
      "source": [
        "def get_soft(string_format):\n",
        "    annotator_dict = {ann_idx:labels_dict[annotation] for ann_idx, annotation in enumerate(string_format.split(',')) if annotation != \"\"}\n",
        "    ann_labs = list(annotator_dict.values())\n",
        "    distr = [ann_labs.count(l) for l in range(len(labels_dict))]\n",
        "    return distr, softmax(distr).tolist()\n",
        "\n",
        "labels_dict = {'ADJ': 0, 'ADP': 1, 'ADV': 2, 'CCONJ': 3, 'DET': 4,'NOUN': 5, 'NUM': 6, 'PART': 8,'PRON': 7,'PUNCT': 9,'VERB': 10, 'X': 11}\n",
        "\n",
        "\n",
        "train_softs = []\n",
        "train_distr = []\n",
        "for line in train_data:\n",
        "    if line:\n",
        "        distr, soft = get_soft(line[-1])\n",
        "        train_softs.append(soft)\n",
        "        train_distr.append(distr)\n",
        "\n",
        "#print((train_softs[5:], train_distr[5:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Training Parameters** "
      ],
      "metadata": {
        "id": "oAhmoMVLAr4-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDSr9m-rKXjV"
      },
      "outputs": [],
      "source": [
        "lstm_size = 200\n",
        "attn_size = 600\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "batsize = 1000\n",
        "sizeout_rate = 0.8\n",
        "\n",
        "word_emb_size = 300\n",
        "char_emb_size = 64\n",
        "\n",
        "word_padsize = word_pad_trn.shape[1]\n",
        "char_padsize = char_pad_trn.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Functions**"
      ],
      "metadata": {
        "id": "7k9iCO7KBF3P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGSAKw5bKXod"
      },
      "outputs": [],
      "source": [
        "def lookup_embeddings(embedding_lookup, index_matrix):\n",
        "    flattened_indices = torch.flatten(index_matrix)\n",
        "    selected = torch.index_select(embedding_lookup, 0, flattened_indices)\n",
        "    return selected.reshape(index_matrix.shape[0], index_matrix.shape[1], embedding_lookup.shape[-1])\n",
        "\n",
        "\n",
        "def to_numpy(torch_tensor):\n",
        "    return torch_tensor.cpu().clone().detach().numpy()\n",
        "\n",
        "def to_cuda(x):\n",
        "    \"\"\" GPU-enable a tensor \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return x\n",
        "\n",
        "def create_dataset(word_pad_trn_bat, word_iis_trn_bat, char_pad_trn_bat, has_labels=False, y_hot_trn_bat=None, y_soft_trn_bat=None):\n",
        "    word_pad_trn_bat, word_iis_trn_bat = torch.from_numpy(word_pad_trn_bat).long().to(device), torch.from_numpy(word_iis_trn_bat).long().to(device)\n",
        "    char_pad_trn_bat = torch.from_numpy(char_pad_trn_bat).long().to(device)\n",
        "    if has_labels:\n",
        "        y_hot_trn_bat = torch.from_numpy(y_hot_trn_bat).float().to(device)\n",
        "        y_soft_trn_bat = torch.from_numpy(y_soft_trn_bat).float().to(device)\n",
        "    return word_pad_trn_bat, word_iis_trn_bat, char_pad_trn_bat, y_hot_trn_bat, y_soft_trn_bat\n",
        "\n",
        "\n",
        "def backprop_hot(optimizer, loss):\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return\n",
        "\n",
        "def get_predictions(model, eval_loader):\n",
        "    hard_preds = []\n",
        "    soft_preds = []\n",
        "    model.eval()\n",
        "    for wwordpad_bat, wwordiis_bat, ccharpad_bat in eval_loader:\n",
        "        one_hot_pred, _ = model(wwordpad_bat, wwordiis_bat, ccharpad_bat, None, None)\n",
        "        one_hot_pred = one_hot_pred.detach().cpu().numpy()\n",
        "        hard_preds.extend(np.argmax(one_hot_pred, 1))\n",
        "        soft_preds.extend(one_hot_pred)\n",
        "    return hard_preds, soft_preds\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating Functions**"
      ],
      "metadata": {
        "id": "MsB1id9qBMmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def cross_entropy_metric(targets, predictions, epsilon=1e-12):\n",
        "    \"\"\"\n",
        "    Computes cross entropy between targets and predictions.\n",
        "    Input: predictions (N, k) ndarray\n",
        "           targets (N, k) ndarray\n",
        "    Returns: scalar\n",
        "    https://stackoverflow.com/questions/47377222/what-is-the-problem-with-my-implementation-of-the-cross-entropy-function\n",
        "    \"\"\"\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    N = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions))/N\n",
        "    return ce\n",
        "\n",
        "\n",
        "def f1_metric(solution, prediction, num_classes=2):\n",
        "    matches, gold, system = {}, {}, {}\n",
        "    for i in range(num_classes):\n",
        "        matches[i] = 0\n",
        "        system[i] = 0\n",
        "        gold[i] = 0\n",
        "\n",
        "    for g, p in zip(solution, prediction):\n",
        "        if p == g:\n",
        "            matches[p] += 1\n",
        "\n",
        "        gold[g] += 1\n",
        "        system[p] += 1\n",
        "\n",
        "    recall = {}\n",
        "    precision = {}\n",
        "    f1 = {}\n",
        "    for i in range(num_classes):\n",
        "        recall[i] = 1.0 * matches[i] / gold[i] if matches[i] != 0 else 0\n",
        "        precision[i] = 1.0 * matches[i] / system[i] if matches[i] !=0 else 0\n",
        "        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0\n",
        "\n",
        "    support = np.array([gold[i] for i in range(num_classes)])\n",
        "\n",
        "    average_recall = np.average([recall[i] for i in range(num_classes)], weights=support)\n",
        "    average_precision = np.average([precision[i] for i in range(num_classes)], weights=support)\n",
        "    average_f1 = np.average([f1[i] for i in range(num_classes)], weights=support)\n",
        "    return average_f1, average_recall, average_precision\n",
        "\n",
        "\n",
        "def load_dictionary(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        dictionary = json.load(f)\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "def get_hard_score(reference_path, submission_path, num_classes):\n",
        "\n",
        "    # submissions are in a dictionary format\n",
        "    reference_dictionary = load_dictionary(reference_path)\n",
        "    submission_dictionary = load_dictionary(submission_path)\n",
        "\n",
        "    # getting the submission vectors\n",
        "    golds = []\n",
        "    predictions = []\n",
        "    for document, doc_contents in reference_dictionary.items():      \n",
        "        sub = submission_dictionary[document]\n",
        "        for item_id, contents in doc_contents.items():\n",
        "            golds.append(contents['gold'])\n",
        "            predictions.append(sub[item_id]['gold'])\n",
        "\n",
        "    #print('Dev reference gold: ',golds)\n",
        "    #print('Dev result gold:', predictions)\n",
        "\n",
        "    f1, recall, precision = f1_metric(np.array(golds), np.array(predictions), num_classes)\n",
        "\n",
        "    return f1, recall, precision\n",
        "\n",
        "\n",
        "def get_soft_score(reference_path, submission_path):\n",
        "\n",
        "    # submissions are in a dictionary format\n",
        "    reference_dictionary = load_dictionary(reference_path)\n",
        "    submission_dictionary = load_dictionary(submission_path)\n",
        "\n",
        "    # getting the submission vectors\n",
        "    softs = []\n",
        "    predictions = []\n",
        "    for document, doc_contents in reference_dictionary.items():\n",
        "        sub = submission_dictionary[document]\n",
        "        for item_id, contents in doc_contents.items():\n",
        "            softs.append(contents['soft'])\n",
        "            predictions.append(sub[item_id]['soft'])\n",
        "    # evaluating using cross_entropy\n",
        "    score = cross_entropy_metric(np.array(softs), np.array(predictions))\n",
        "    return score"
      ],
      "metadata": {
        "id": "ujq8w14pLOpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Getting, Tensorizing and Batching the Data**"
      ],
      "metadata": {
        "id": "d9GmT73DBaF4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKdX4O4dKXrG"
      },
      "outputs": [],
      "source": [
        "train_softs = np.array(train_softs)\n",
        "\n",
        "train_softs.shape\n",
        "\n",
        "train_mv = np.argmax(train_softs, 1)\n",
        "\n",
        "word_pad_dev_tens, word_iis_dev_tens, char_pad_dev_tens, hot_dev_tens, soft_dev_tens = create_dataset(word_pad_dev, word_iis_dev, char_pad_dev)\n",
        "dev = data_utils.TensorDataset(word_pad_dev_tens, word_iis_dev_tens, char_pad_dev_tens)\n",
        "dev_loader = data_utils.DataLoader(dev, batch_size=batsize, shuffle=False)\n",
        "\n",
        "word_pad_trn_tens, word_iis_trn_tens, char_pad_trn_ten, hot_trn_tens, soft_trn_tens = create_dataset(word_pad_trn, word_iis_trn, char_pad_trn, True, train_mv, train_softs)\n",
        "train = data_utils.TensorDataset(word_pad_trn_tens, word_iis_trn_tens, char_pad_trn_ten, hot_trn_tens, soft_trn_tens)\n",
        "train_loader = data_utils.DataLoader(train, batch_size=batsize, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "QpeRXYzcBdhD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVv-O7OQKXtm"
      },
      "outputs": [],
      "source": [
        "class Word_Encoder(torch.nn.Module):\n",
        "    def __init__(self, lstm_size, embedding_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bilstm = torch.nn.LSTM(embedding_size, lstm_size, bidirectional=True, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, wword_pad, col_indices):\n",
        "        embedded_words = lookup_embeddings(word_embedding_lookup, wword_pad)\n",
        "        rnn_context, _ = self.bilstm(embedded_words)\n",
        "        rnn_sequence = torch.stack([torch.index_select(seq, 0, i) for seq, i in zip(rnn_context, col_indices)], 0)\n",
        "        rnn_sequence = self.dropout(rnn_sequence)\n",
        "        return rnn_sequence, rnn_context\n",
        "\n",
        "\n",
        "class Char_Encoder(torch.nn.Module):\n",
        "    def __init__(self, lstm_size, embedding_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bilstm = torch.nn.LSTM(embedding_size, lstm_size, bidirectional=True, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, cchar_pad):\n",
        "        embedded_chars = lookup_embeddings(char_embedding_lookup, cchar_pad)\n",
        "        rnn_sequence, _ = self.bilstm(embedded_chars)\n",
        "        rnn_sequence = self.dropout(rnn_sequence[:,1])\n",
        "        reshaped = torch.reshape(rnn_sequence, [-1, 1, rnn_sequence.shape[1]])\n",
        "        return reshaped\n",
        "\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, attn_emb_dim, attn_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn_nn = torch.nn.Sequential(\n",
        "                    torch.nn.Linear(attn_emb_dim, attn_size),\n",
        "                    torch.nn.ReLU())\n",
        "\n",
        "        self.u_omega = torch.nn.Parameter(torch.randn([attn_size]))\n",
        "\n",
        "    def forward(self, attn_in, s):\n",
        "        v = self.attn_nn(attn_in)\n",
        "        vu = torch.matmul(v.squeeze(1), self.u_omega)\n",
        "        alphas = torch.nn.functional.softmax(vu, 0)\n",
        "        final = torch.sum(attn_in * alphas.unsqueeze(-1), 1)\n",
        "        return final\n",
        "\n",
        "\n",
        "\n",
        "class RNN_all(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.word_encoder = Word_Encoder(lstm_size, word_emb_size)\n",
        "        self.char_encoder = Char_Encoder(lstm_size, char_emb_size)\n",
        "\n",
        "        self.char_attention = Attention(lstm_size*2, attn_size)\n",
        "        self.word_attention = Attention(lstm_size*2, attn_size)\n",
        "\n",
        "        concat_size = lstm_size*4\n",
        "        hidden1 = int(lstm_size*4*sizeout_rate)\n",
        "        out_final = int(hidden1*sizeout_rate)\n",
        "        self.fulcon = torch.nn.Sequential(\n",
        "                    torch.nn.Linear(concat_size, hidden1),\n",
        "                    torch.nn.Linear(hidden1, out_final))\n",
        "\n",
        "        self.output_hot = torch.nn.Linear(out_final, hotsize)\n",
        "    \n",
        "    def forward(self, wword_pad, wword_iis, cchar_pad, one_hot_labels, soft_labels, eval=True):\n",
        "        word_sequence, word_context = self.word_encoder(wword_pad, wword_iis)\n",
        "        word_attn= self.word_attention(word_sequence, 'word')\n",
        "\n",
        "        char_sequence = self.char_encoder(cchar_pad)\n",
        "        char_attn = self.char_attention(char_sequence, 'char')\n",
        "\n",
        "        concat_attn = torch.cat([word_attn, char_attn], 1)\n",
        "        ful = self.fulcon(concat_attn)\n",
        "\n",
        "        pred_hot = self.output_hot(ful)  \n",
        "        softmax_scores = torch.nn.functional.softmax(pred_hot, 1) + 1e-43\n",
        "        if eval:\n",
        "            return softmax_scores, None\n",
        "        else:\n",
        "            soft_labels = soft_labels + 1e-43\n",
        "            softmax_scores = torch.nn.functional.softmax(pred_hot, 1) + 1e-4\n",
        "            cross_entropy = torch.mul(soft_labels, softmax_scores.log())\n",
        "            loss  = -torch.sum(cross_entropy)\n",
        "            return softmax_scores, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Training"
      ],
      "metadata": {
        "id": "U_p83VmqBuxx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFzg3I6VKXwF",
        "outputId": "dfb58f14-29a5-467f-b79e-e07d945660e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning the Training\n",
            "\n",
            "Experiment 0 #######################\n",
            "Epoch: 01 | Epoch Time: 1m 40s\n",
            "Training Loss: 1997.7433471679688 | Training Acc: 0.337150514125824\n",
            "------------------------------------------------------------\n",
            "Epoch: 02 | Epoch Time: 1m 32s\n",
            "Training Loss: 1511.5850931803386 | Training Acc: 0.5724623203277588\n",
            "------------------------------------------------------------\n",
            "Epoch: 03 | Epoch Time: 1m 33s\n",
            "Training Loss: 1315.7501152886284 | Training Acc: 0.7008172273635864\n",
            "------------------------------------------------------------\n",
            "Epoch: 04 | Epoch Time: 1m 36s\n",
            "Training Loss: 1197.2987331814236 | Training Acc: 0.757544755935669\n",
            "------------------------------------------------------------\n",
            "Epoch: 05 | Epoch Time: 1m 35s\n",
            "Training Loss: 1115.631544325087 | Training Acc: 0.8003333210945129\n",
            "------------------------------------------------------------\n",
            "Epoch: 06 | Epoch Time: 1m 36s\n",
            "Training Loss: 1061.4736633300781 | Training Acc: 0.8332903385162354\n",
            "------------------------------------------------------------\n",
            "Epoch: 07 | Epoch Time: 1m 35s\n",
            "Training Loss: 1027.7395697699653 | Training Acc: 0.8484874963760376\n",
            "------------------------------------------------------------\n",
            "Epoch: 08 | Epoch Time: 1m 34s\n",
            "Training Loss: 998.5251397026909 | Training Acc: 0.8625770807266235\n",
            "------------------------------------------------------------\n",
            "Epoch: 09 | Epoch Time: 1m 36s\n",
            "Training Loss: 977.5882195366753 | Training Acc: 0.8765304088592529\n",
            "------------------------------------------------------------\n",
            "Epoch: 10 | Epoch Time: 1m 35s\n",
            "Training Loss: 959.7301262749565 | Training Acc: 0.8849390149116516\n",
            "------------------------------------------------------------\n",
            "Epoch: 11 | Epoch Time: 1m 35s\n",
            "Training Loss: 946.2049899631077 | Training Acc: 0.8955591917037964\n",
            "------------------------------------------------------------\n",
            "Epoch: 12 | Epoch Time: 1m 37s\n",
            "Training Loss: 932.829369439019 | Training Acc: 0.8972688317298889\n",
            "------------------------------------------------------------\n",
            "Epoch: 13 | Epoch Time: 1m 37s\n",
            "Training Loss: 922.7929755316841 | Training Acc: 0.9039497375488281\n",
            "------------------------------------------------------------\n",
            "Epoch: 14 | Epoch Time: 1m 37s\n",
            "Training Loss: 914.0929022894966 | Training Acc: 0.9144120812416077\n",
            "------------------------------------------------------------\n",
            "Epoch: 15 | Epoch Time: 1m 36s\n",
            "Training Loss: 905.9566040039062 | Training Acc: 0.9159032106399536\n",
            "------------------------------------------------------------\n",
            "Epoch: 16 | Epoch Time: 1m 37s\n",
            "Training Loss: 901.9324340820312 | Training Acc: 0.9160250425338745\n",
            "------------------------------------------------------------\n",
            "Epoch: 17 | Epoch Time: 1m 37s\n",
            "Training Loss: 899.8323838975695 | Training Acc: 0.9201361536979675\n",
            "------------------------------------------------------------\n",
            "Epoch: 18 | Epoch Time: 1m 36s\n",
            "Training Loss: 891.941158718533 | Training Acc: 0.9257813692092896\n",
            "------------------------------------------------------------\n",
            "Epoch: 19 | Epoch Time: 1m 36s\n",
            "Training Loss: 888.176988389757 | Training Acc: 0.9281541109085083\n",
            "------------------------------------------------------------\n",
            "Epoch: 20 | Epoch Time: 1m 36s\n",
            "Training Loss: 882.5464714898003 | Training Acc: 0.9311756491661072\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\"\"\"**Training using the soft labels**\"\"\"\n",
        "\n",
        "hotsize = 12\n",
        "assert len(train_softs[0]) == hotsize\n",
        "\n",
        "word_embedding_lookup = torch.from_numpy(word_emb).float().to(device)\n",
        "char_embedding_lookup = torch.from_numpy(char_emb).float().to(device)\n",
        "\n",
        "print('Beginning the Training')\n",
        "NUM_EXPERIMENTS = 1\n",
        "num_epochs = 20\n",
        "\n",
        "accs = []\n",
        "prfs = []\n",
        "ct_prfs = []\n",
        "jsds = []\n",
        "kls = []\n",
        "similarity_ents = []\n",
        "ents_correlation = []\n",
        "ce_results = []\n",
        "\n",
        "dev_accs = []\n",
        "dev_prfs = []\n",
        "\n",
        "train_losses=[]\n",
        "\n",
        "for exp in range(NUM_EXPERIMENTS):\n",
        "    print('\\nExperiment %d #######################'%exp)\n",
        "    best_val_f, best_val_acc = 0, 0\n",
        "    best_val_r, best_val_p = 0, 0\n",
        "\n",
        "    last_batch = 0\n",
        "\n",
        "    model = RNN_all()\n",
        "    model = to_cuda(model)\n",
        "    optimizer = torch.optim.Adam(params=[p for p in model.parameters()],lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0\n",
        "        acc, dev_acc= 0 , 0\n",
        "        nepoch = epoch + 1\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        for word_pad_trn_bat, word_iis_trn_bat, char_pad_trn_bat, y_hot_trn_bat, y_soft_trn_bat in train_loader:\n",
        "            hard_predictions, hard_loss = model(word_pad_trn_bat, word_iis_trn_bat, char_pad_trn_bat, y_hot_trn_bat, y_soft_trn_bat, False)\n",
        "            backprop_hot(optimizer, hard_loss)\n",
        "            running_loss += hard_loss.item()            \n",
        "\n",
        "            # as Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "            ps = torch.exp(hard_predictions)\n",
        "            top_p , top_class = ps.topk(1,dim=1)\n",
        "            equals = top_class == y_hot_trn_bat.view(*top_class.shape)\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc+= torch.mean(equals.type(torch.FloatTensor))\n",
        "            \n",
        "                    \n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "        accs.append(acc/len(train_loader))\n",
        "        print(f'Training Loss: {train_losses[epoch]} | Training Acc: {accs[epoch].item()}')             \n",
        "\n",
        "        # evaluate after each epoch using \n",
        "        dev_hard_preds, dev_soft_preds = get_predictions(model, dev_loader)\n",
        "    \n",
        "        print('------------------------------------------------------------')\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Summary"
      ],
      "metadata": {
        "id": "g-NSzSxOBqir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsNMRYLAM0bg",
        "outputId": "6d4187f8-40cd-446b-9f04-e993e07041f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN_all(\n",
            "  (word_encoder): Word_Encoder(\n",
            "    (bilstm): LSTM(300, 200, batch_first=True, bidirectional=True)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (char_encoder): Char_Encoder(\n",
            "    (bilstm): LSTM(64, 200, batch_first=True, bidirectional=True)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (char_attention): Attention(\n",
            "    (attn_nn): Sequential(\n",
            "      (0): Linear(in_features=400, out_features=600, bias=True)\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (word_attention): Attention(\n",
            "    (attn_nn): Sequential(\n",
            "      (0): Linear(in_features=400, out_features=600, bias=True)\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (fulcon): Sequential(\n",
            "    (0): Linear(in_features=800, out_features=640, bias=True)\n",
            "    (1): Linear(in_features=640, out_features=512, bias=True)\n",
            "  )\n",
            "  (output_hot): Linear(in_features=512, out_features=12, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating model on Dev data**"
      ],
      "metadata": {
        "id": "lyAKIEsAB82F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"**Putting the data in the codalab answer format**\"\"\"\n",
        "#Gimpel-POS_answers.jsonlines- dev file\n",
        "\n",
        "codalab_dict = {str(i): {\"gold\": int(dev_hard_preds[i]), \"soft\": pred.tolist()} for i, pred in enumerate(dev_soft_preds)}\n",
        "codalab_dict = {\"dummy_name\": codalab_dict} \n",
        "\n",
        "with open(DATA_PATH +'/Gimpel-POS_answers-dev.jsonlines', 'w') as f:\n",
        "    json.dump(codalab_dict, f)"
      ],
      "metadata": {
        "id": "sGBVDBdvMLGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of dev reference file\n",
        "dev_ref_path ='/content/drive/MyDrive/MscThesis/dev_reference_labels/Gimpel-POS_answers.jsonlines'\n",
        "# Path of dev result file\n",
        "dev_res_path= '/content/drive/MyDrive/MscThesis/public_data_PP/gimpel_pos/Gimpel-POS_answers-dev.jsonlines'\n",
        "\n",
        "num_classes = 12\n",
        "f1, recall, precision = get_hard_score(dev_ref_path, dev_res_path, num_classes)\n",
        "soft_score_dev = get_soft_score(dev_ref_path, dev_res_path)\n",
        "print('Dev Baseline Model')\n",
        "print(f'Precision:{precision*100 : .2f}% | Recall: {recall*100: .2f}% ')\n",
        "print(f'F1 scores: {f1} | Cross-Entropy: {soft_score_dev}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OAX6Qc4Ph7x",
        "outputId": "fa5531aa-a0cc-47ef-95f4-a537a074903d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Baseline Model\n",
            "Precision: 78.87% | Recall:  78.56% \n",
            "F1 scores: 0.7733012051352617 | Cross-Entropy: 1.0884745740067088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating model on Test data**"
      ],
      "metadata": {
        "id": "uL2KLAYsCH_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Importing Test Data\"\"\"\n",
        "# the test data from csv\n",
        "test_path = '/content/drive/MyDrive/MscThesis/public_data_evaluation/gimpel_pos/test_data.tsv'\n",
        "test_data = read_csv_data(test_path)\n",
        "\n",
        "# the indices of the words in a sentence, saved as arrays. Hint:Helps you know where each sentence ends\n",
        "test_iis_path = '/content/drive/MyDrive/MscThesis/public_data_evaluation/gimpel_pos/word_iis_tst.npy'\n",
        "word_iis_test = np.load(test_iis_path)\n",
        "\n",
        "# the padded sentences (maximum of 40 words per sentence). Two words in the same sentence will have the same word_pad. The numbers\n",
        "# indicate the idx of the word in the word embedding dictionary.\n",
        "test_wordpad_path = '/content/drive/MyDrive/MscThesis/public_data_evaluation/gimpel_pos/word_pad_tst.npy'\n",
        "word_pad_test = np.load(test_wordpad_path)\n",
        "\n",
        "# the character padding\n",
        "test_charpad_path = '/content/drive/MyDrive/MscThesis/public_data_evaluation/gimpel_pos/char_pad_tst.npy'\n",
        "char_pad_test = np.load(test_charpad_path)\n",
        "\n",
        "#\"\"\"**Getting, Tensorizing and Batching the Data**\"\"\"\n",
        "word_pad_tst_tens, word_iis_tst_tens, char_pad_tst_tens, hot_tst_tens, soft_tst_tens = create_dataset(word_pad_test, word_iis_test, char_pad_test)\n",
        "test = data_utils.TensorDataset(word_pad_tst_tens, word_iis_tst_tens, char_pad_tst_tens)\n",
        "test_loader = data_utils.DataLoader(test, batch_size=batsize, shuffle=False)\n"
      ],
      "metadata": {
        "id": "-QT1PnBwnj4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_hard_preds, test_soft_preds = get_predictions(model, test_loader)"
      ],
      "metadata": {
        "id": "feVifWjWYS5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BJshomSKXyn"
      },
      "outputs": [],
      "source": [
        "\"\"\"**Putting the data in the codalab answer format**\"\"\"\n",
        "#Gimpel-POS_answers.jsonlines- test file\n",
        "\n",
        "codalab_dict = {str(i): {\"gold\": int(test_hard_preds[i]), \"soft\": pred.tolist()} for i, pred in enumerate(test_soft_preds)}\n",
        "codalab_dict = {\"dummy_name\": codalab_dict} \n",
        "with open(DATA_PATH +'/Gimpel-POS_answers-test.jsonlines', 'w') as f:\n",
        "    json.dump(codalab_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of test reference file\n",
        "test_ref_path= '/content/drive/MyDrive/MscThesis/test_reference_data/Gimpel-POS_answers.jsonlines'\n",
        "# Path of test result file\n",
        "test_res_path= '/content/drive/MyDrive/MscThesis/public_data_PP/gimpel_pos/Gimpel-POS_answers-test.jsonlines'\n",
        "\n",
        "num_classes = 12\n",
        "f1, recall, precision  = get_hard_score(test_ref_path, test_res_path, num_classes)\n",
        "soft_score = get_soft_score(test_ref_path, test_res_path)\n",
        "\n",
        "print('Test Baseline Model')\n",
        "print(f'Precision:{precision*100 : .2f}% | Recall: {recall*100: .2f}% ')\n",
        "print(f'F1 scores: {f1} | Cross-Entropy: {soft_score_dev}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-mYpJuDPdsM",
        "outputId": "1677118d-0bb9-42ae-9c8d-b8eaf20323e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Baseline Model\n",
            "Precision: 77.69% | Recall:  77.37% \n",
            "F1 scores: 0.7635734636950491 | Cross-Entropy: 1.0884745740067088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **References**"
      ],
      "metadata": {
        "id": "6sNAUogaeYvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Pytorch.org. 2022. PyTorch documentation — PyTorch 1.12 documentation. [online] Available at: <https://pytorch.org/docs/stable/index.html> [Accessed 21 August 2022].\n",
        "\n",
        "*  Numpy.org. 2022. NumPy documentation — NumPy v1.23 Manual. [online] Available at: <https://numpy.org/doc/stable/> [Accessed 21 August 2022].\n",
        "\n",
        "*   Spacy.io. 2022. spaCy- Documentation. [online] Available at: <https://spacy.io/api> [Accessed 21 August 2022].\n",
        "\n",
        "*   Pytorch.org. 2022. GRU — PyTorch 1.12 documentation. [online] Available at: <https://pytorch.org/docs/stable/generated/torch.nn.GRU.html> [Accessed 21 August 2022].\n",
        "\n",
        "*  Pytorch.org. 2022. RNN — PyTorch 1.12 documentation. [online] Available at: <https://pytorch.org/docs/stable/generated/torch.nn.RNN.html> [Accessed 21 August 2022].\n",
        "\n",
        "*  Stack Overflow. 2022. Stack Overflow - Where Developers Learn, Share, & Build Careers. [online] Available at: <https://stackoverflow.com/> [Accessed 21 August 2022].\n"
      ],
      "metadata": {
        "id": "7pJzOXUOeb9O"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "POS_LSTM_Relu()",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}