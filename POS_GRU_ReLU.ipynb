{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required libraries"
      ],
      "metadata": {
        "id": "lSoj-Fqve_1Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7YyUbWTIatX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import csv\n",
        "import torch.utils.data as data_utils\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "from scipy.special import softmax\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTBo2SFyScrI",
        "outputId": "c67ad267-7fea-42ad-b722-01b836b0f0d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Importing and Data Preparation"
      ],
      "metadata": {
        "id": "8QKYbrf8fGla"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwgFgYLoKXWH"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/MscThesis/public_data_PP/gimpel_pos'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reading in the data**"
      ],
      "metadata": {
        "id": "9sZxVVBifKjM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRpWocMXKXdc"
      },
      "outputs": [],
      "source": [
        "def read_csv_data(path):\n",
        "  '''\n",
        "  User define function to read the .csv file from the given path\n",
        "  '''\n",
        "  data = []\n",
        "  with open(path, 'r') as f:\n",
        "    tsvreader = csv.reader(f, delimiter=\"\\t\")\n",
        "    for line in tsvreader:\n",
        "      data.append(line)\n",
        "    # to remove the header\n",
        "  return data[1:]\n",
        "\n",
        "\n",
        "def readbin(f_in):\n",
        "  '''\n",
        "  User define function to read the .bin file from the given path\n",
        "  '''\n",
        "  inp = open(f_in, \"rb\")\n",
        "  out = pickle.load(inp)\n",
        "  inp.close()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GU_seH2KXgW"
      },
      "outputs": [],
      "source": [
        "# prepared word embeddings for the model.\n",
        "word_emb_matrix = DATA_PATH + '/embeddings/word_emb_matrix.bin'\n",
        "word_emb = readbin(word_emb_matrix)\n",
        "\n",
        "# prepared character embeddings for the model\n",
        "char_emb_matrix = DATA_PATH + '/embeddings/char_emb_matrix.bin'\n",
        "char_emb = readbin(char_emb_matrix)\n",
        "\n",
        "# the train and dev data from csv\n",
        "train_path = DATA_PATH + '/input_data/train_data.tsv'\n",
        "train_data = read_csv_data(train_path)\n",
        "\n",
        "dev_path = DATA_PATH + '/input_data/dev_data.tsv'\n",
        "dev_data = read_csv_data(dev_path)\n",
        "\n",
        "# the indices of the words in a sentence, saved as arrays. Hint:Helps you know where each sentence ends\n",
        "train_iis_path = DATA_PATH + '/input_data/word_iis_trn.npy'\n",
        "dev_iis_path = DATA_PATH + '/input_data/word_iis_dev.npy'\n",
        "\n",
        "word_iis_trn = np.load(train_iis_path)\n",
        "word_iis_dev = np.load(dev_iis_path)\n",
        "\n",
        "# the padded sentences (maximum of 40 words per sentence). Two words in the same sentence will have the same word_pad. The numbers\n",
        "# indicate the idx of the word in the word embedding dictionary.\n",
        "train_wordpad_path = DATA_PATH + '/input_data/word_pad_trn.npy'\n",
        "dev_wordpad_path = DATA_PATH + '/input_data/word_pad_dev.npy'\n",
        "\n",
        "word_pad_trn = np.load(train_wordpad_path)\n",
        "word_pad_dev = np.load(dev_wordpad_path)\n",
        "\n",
        "# the character padding\n",
        "train_charpad_path = DATA_PATH + '/input_data/char_pad_trn.npy'\n",
        "dev_charpad_path = DATA_PATH + '/input_data/char_pad_dev.npy'\n",
        "\n",
        "char_pad_trn = np.load(train_charpad_path)\n",
        "char_pad_dev = np.load(dev_charpad_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Getting the training labels**"
      ],
      "metadata": {
        "id": "3zlZs_dffP_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctDotC_mKXl9"
      },
      "outputs": [],
      "source": [
        "def get_soft(string_format):\n",
        "    annotator_dict = {ann_idx:labels_dict[annotation] for ann_idx, annotation in enumerate(string_format.split(',')) if annotation != \"\"}\n",
        "    ann_labs = list(annotator_dict.values())\n",
        "    distr = [ann_labs.count(l) for l in range(len(labels_dict))]\n",
        "    return distr, softmax(distr).tolist()\n",
        "\n",
        "labels_dict = {'ADJ': 0, 'ADP': 1, 'ADV': 2, 'CCONJ': 3, 'DET': 4,'NOUN': 5, 'NUM': 6, 'PART': 8,'PRON': 7,'PUNCT': 9,'VERB': 10, 'X': 11}\n",
        "\n",
        "\n",
        "train_softs = []\n",
        "train_distr = []\n",
        "for line in train_data:\n",
        "    if line:\n",
        "        distr, soft = get_soft(line[-1])\n",
        "        train_softs.append(soft)\n",
        "        train_distr.append(distr)\n",
        "\n",
        "#print((train_softs[5:], train_distr[5:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Training Parameters** "
      ],
      "metadata": {
        "id": "I43Xm_a1fVvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDSr9m-rKXjV"
      },
      "outputs": [],
      "source": [
        "gru_size = 100\n",
        "attn_size = 400\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "batsize = 1000\n",
        "sizeout_rate = 0.8\n",
        "\n",
        "word_emb_size = 300\n",
        "char_emb_size = 64\n",
        "\n",
        "word_padsize = word_pad_trn.shape[1]\n",
        "char_padsize = char_pad_trn.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Training Functions** "
      ],
      "metadata": {
        "id": "uYYdhFltfZAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGSAKw5bKXod"
      },
      "outputs": [],
      "source": [
        "#Embedding lookup used for word and character embeddings\n",
        "def lookup_embeddings(embedding_lookup, index_matrix):\n",
        "    flattened_indices = torch.flatten(index_matrix)\n",
        "    selected = torch.index_select(embedding_lookup, 0, flattened_indices)\n",
        "    return selected.reshape(index_matrix.shape[0], index_matrix.shape[1], embedding_lookup.shape[-1])\n",
        "\n",
        "# tensorizing the data\n",
        "def to_numpy(torch_tensor):\n",
        "    return torch_tensor.cpu().clone().detach().numpy()\n",
        "\n",
        "# to check GPU availability\n",
        "def to_cuda(x):\n",
        "    \"\"\" GPU-enable a tensor \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return x\n",
        "\n",
        "#Creating Tensor for dataloder\n",
        "def create_dataset(word_pad_trn_bat, word_iis_trn_bat, char_pad_trn_bat, has_labels=False, y_hot_trn_bat=None, y_soft_trn_bat=None):\n",
        "    word_pad_trn_bat, word_iis_trn_bat = torch.from_numpy(word_pad_trn_bat).long().to(device), torch.from_numpy(word_iis_trn_bat).long().to(device)\n",
        "    char_pad_trn_bat = torch.from_numpy(char_pad_trn_bat).long().to(device)\n",
        "    if has_labels:\n",
        "        y_hot_trn_bat = torch.from_numpy(y_hot_trn_bat).float().to(device)\n",
        "        y_soft_trn_bat = torch.from_numpy(y_soft_trn_bat).float().to(device)\n",
        "    return word_pad_trn_bat, word_iis_trn_bat, char_pad_trn_bat, y_hot_trn_bat, y_soft_trn_bat\n",
        "\n",
        "# to initialize to the optimizer\n",
        "def backprop_hot(optimizer, loss):\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return\n",
        "\n",
        "# to get predection from the model\n",
        "def get_predictions(model, eval_loader):\n",
        "    hard_preds = []\n",
        "    soft_preds = []\n",
        "    model.eval()\n",
        "    for wwordpad_bat, wwordiis_bat, ccharpad_bat in eval_loader:\n",
        "        one_hot_pred, _ = model(wwordpad_bat, wwordiis_bat, ccharpad_bat, None, None)\n",
        "        one_hot_pred = one_hot_pred.detach().cpu().numpy()\n",
        "        hard_preds.extend(np.argmax(one_hot_pred, 1))\n",
        "        soft_preds.extend(one_hot_pred)\n",
        "    return hard_preds, soft_preds\n",
        "\n",
        "# to calculate each epoch time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Evaluating Functions** "
      ],
      "metadata": {
        "id": "G_CQI1CVfiFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To calculate the cross entropy for soft evaluation\n",
        "def cross_entropy_metric(targets, predictions, epsilon=1e-12):\n",
        "    \"\"\"\n",
        "    Computes cross entropy between targets and predictions.\n",
        "    Input: predictions (N, k) ndarray\n",
        "           targets (N, k) ndarray\n",
        "    Returns: scalar\n",
        "    https://stackoverflow.com/questions/47377222/what-is-the-problem-with-my-implementation-of-the-cross-entropy-function\n",
        "    \"\"\"\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    N = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions))/N\n",
        "    return ce\n",
        "\n",
        "# To calculate F1- scores given actuall and predictions for hard evaluation\n",
        "def f1_metric(solution, prediction, num_classes=2):\n",
        "    matches, gold, system = {}, {}, {}\n",
        "    for i in range(num_classes):\n",
        "        matches[i] = 0\n",
        "        system[i] = 0\n",
        "        gold[i] = 0\n",
        "\n",
        "    for g, p in zip(solution, prediction):\n",
        "        if p == g:\n",
        "            matches[p] += 1\n",
        "\n",
        "        gold[g] += 1\n",
        "        system[p] += 1\n",
        "\n",
        "    recall = {}\n",
        "    precision = {}\n",
        "    f1 = {}\n",
        "    for i in range(num_classes):\n",
        "        recall[i] = 1.0 * matches[i] / gold[i] if matches[i] != 0 else 0\n",
        "        precision[i] = 1.0 * matches[i] / system[i] if matches[i] !=0 else 0\n",
        "        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0\n",
        "\n",
        "    support = np.array([gold[i] for i in range(num_classes)])\n",
        "\n",
        "    average_recall = np.average([recall[i] for i in range(num_classes)], weights=support)\n",
        "    average_precision = np.average([precision[i] for i in range(num_classes)], weights=support)\n",
        "    average_f1 = np.average([f1[i] for i in range(num_classes)], weights=support)\n",
        "    return average_f1, average_recall, average_precision\n",
        "\n",
        "# to load the jsonaline files\n",
        "def load_dictionary(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        dictionary = json.load(f)\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "def get_hard_score(reference_path, submission_path, num_classes):\n",
        "\n",
        "    # submissions are in a dictionary format\n",
        "    reference_dictionary = load_dictionary(reference_path)\n",
        "    submission_dictionary = load_dictionary(submission_path)\n",
        "\n",
        "    # getting the submission vectors\n",
        "    golds = []\n",
        "    predictions = []\n",
        "    for document, doc_contents in reference_dictionary.items():      \n",
        "        sub = submission_dictionary[document]\n",
        "        for item_id, contents in doc_contents.items():\n",
        "            golds.append(contents['gold'])\n",
        "            predictions.append(sub[item_id]['gold'])\n",
        "\n",
        "    #print('Dev reference gold: ',golds)\n",
        "    #print('Dev result gold:', predictions)\n",
        "\n",
        "    f1, recall, precision = f1_metric(np.array(golds), np.array(predictions), num_classes)\n",
        "\n",
        "    return f1, recall, precision\n",
        "\n",
        "\n",
        "def get_soft_score(reference_path, submission_path):\n",
        "\n",
        "    # submissions are in a dictionary format\n",
        "    reference_dictionary = load_dictionary(reference_path)\n",
        "    submission_dictionary = load_dictionary(submission_path)\n",
        "\n",
        "    # getting the submission vectors\n",
        "    softs = []\n",
        "    predictions = []\n",
        "    for document, doc_contents in reference_dictionary.items():\n",
        "        sub = submission_dictionary[document]\n",
        "        for item_id, contents in doc_contents.items():\n",
        "            softs.append(contents['soft'])\n",
        "            predictions.append(sub[item_id]['soft'])\n",
        "    # evaluating using cross_entropy\n",
        "    score = cross_entropy_metric(np.array(softs), np.array(predictions))\n",
        "    return score"
      ],
      "metadata": {
        "id": "ujq8w14pLOpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Getting, Tensorizing and Batching the Data**"
      ],
      "metadata": {
        "id": "2RAa2gpNfzMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKdX4O4dKXrG"
      },
      "outputs": [],
      "source": [
        "train_softs = np.array(train_softs)\n",
        "train_softs.shape\n",
        "train_mv = np.argmax(train_softs, 1)\n",
        "\n",
        "#Creating Dev Dataloader\n",
        "word_pad_dev_tens, word_iis_dev_tens, char_pad_dev_tens, hot_dev_tens, soft_dev_tens = create_dataset(word_pad_dev, word_iis_dev, char_pad_dev)\n",
        "dev = data_utils.TensorDataset(word_pad_dev_tens, word_iis_dev_tens, char_pad_dev_tens)\n",
        "dev_loader = data_utils.DataLoader(dev, batch_size=batsize, shuffle=False)\n",
        "\n",
        "#Creating Train Dataloader\n",
        "word_pad_trn_tens, word_iis_trn_tens, char_pad_trn_ten, hot_trn_tens, soft_trn_tens = create_dataset(word_pad_trn, word_iis_trn, char_pad_trn, True, train_mv, train_softs)\n",
        "train = data_utils.TensorDataset(word_pad_trn_tens, word_iis_trn_tens, char_pad_trn_ten, hot_trn_tens, soft_trn_tens)\n",
        "train_loader = data_utils.DataLoader(train, batch_size=batsize, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model**"
      ],
      "metadata": {
        "id": "vhBv3KJwf3ri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVv-O7OQKXtm"
      },
      "outputs": [],
      "source": [
        "#Defining the Word Encoder\n",
        "class Word_Encoder(torch.nn.Module):\n",
        "    def __init__(self, gru_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.gru = torch.nn.GRU(embedding_size, gru_size, bidirectional=True, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, wword_pad, col_indices):\n",
        "        embedded_words = lookup_embeddings(word_embedding_lookup, wword_pad)\n",
        "        rnn_context, _ = self.gru(embedded_words)\n",
        "        rnn_sequence = torch.stack([torch.index_select(seq, 0, i) for seq, i in zip(rnn_context, col_indices)], 0)\n",
        "        rnn_sequence = self.dropout(rnn_sequence)\n",
        "        return rnn_sequence, rnn_context\n",
        "\n",
        "#Defining the Character Encoder\n",
        "class Char_Encoder(torch.nn.Module):\n",
        "    def __init__(self, gru_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.gru = torch.nn.GRU(embedding_size, gru_size, bidirectional=True, batch_first=True)\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, cchar_pad):\n",
        "        embedded_chars = lookup_embeddings(char_embedding_lookup, cchar_pad)\n",
        "        rnn_sequence, _ = self.gru(embedded_chars)\n",
        "        rnn_sequence = self.dropout(rnn_sequence[:,1])\n",
        "        reshaped = torch.reshape(rnn_sequence, [-1, 1, rnn_sequence.shape[1]])\n",
        "        return reshaped\n",
        "\n",
        "#Defining a seperate Attention\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, attn_emb_dim, attn_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn_nn = torch.nn.Sequential(\n",
        "                    torch.nn.Linear(attn_emb_dim, attn_size),\n",
        "                    torch.nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        self.u_omega = torch.nn.Parameter(torch.randn([attn_size]))\n",
        "\n",
        "    def forward(self, attn_in, s):\n",
        "        v = self.attn_nn(attn_in)\n",
        "        vu = torch.matmul(v.squeeze(1), self.u_omega)\n",
        "        alphas = torch.nn.functional.softmax(vu, 0)\n",
        "        final = torch.sum(attn_in * alphas.unsqueeze(-1), 1)\n",
        "        return final\n",
        "\n",
        "# Defining the finale RNN architecture\n",
        "class RNN_all(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.word_encoder = Word_Encoder(gru_size, word_emb_size)\n",
        "        self.char_encoder = Char_Encoder(gru_size, char_emb_size)\n",
        "\n",
        "        self.char_attention = Attention(gru_size*2, attn_size)\n",
        "        self.word_attention = Attention(gru_size*2, attn_size)\n",
        "\n",
        "        concat_size = gru_size*4\n",
        "        hidden1 = int(gru_size*4*sizeout_rate)\n",
        "        out_final = int(hidden1*sizeout_rate)\n",
        "        self.fulcon = torch.nn.Sequential(\n",
        "                    torch.nn.Linear (concat_size, hidden1),\n",
        "                    torch.nn.Linear(hidden1, out_final))\n",
        "\n",
        "        self.output_hot = torch.nn.Linear(out_final, hotsize)\n",
        "        \n",
        "    \n",
        "    def forward(self, wword_pad, wword_iis, cchar_pad, one_hot_labels, soft_labels, eval=True):\n",
        "        word_sequence, word_context = self.word_encoder(wword_pad, wword_iis)\n",
        "        word_attn= self.word_attention(word_sequence, 'word')\n",
        "\n",
        "        char_sequence = self.char_encoder(cchar_pad)\n",
        "        char_attn = self.char_attention(char_sequence, 'char')\n",
        "\n",
        "        concat_attn = torch.cat([word_attn, char_attn], 1)\n",
        "        ful = self.fulcon(concat_attn)\n",
        "\n",
        "        pred_hot = self.output_hot(ful)  \n",
        "        softmax_scores = torch.nn.functional.softmax(pred_hot, 1) + 1e-43\n",
        "        if eval:\n",
        "            return softmax_scores, None\n",
        "        else:\n",
        "            soft_labels = soft_labels + 1e-43\n",
        "            softmax_scores = torch.nn.functional.softmax(pred_hot, 1) + 1e-4\n",
        "            cross_entropy = torch.mul(soft_labels, softmax_scores.log())\n",
        "            loss  = -torch.sum(cross_entropy)\n",
        "            return softmax_scores, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Training"
      ],
      "metadata": {
        "id": "7HkuK5ASgBFK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFzg3I6VKXwF",
        "outputId": "b0de1798-723a-4ec8-a3df-d2a1b346b421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning the Training\n",
            "\n",
            "Experiment 0 #######################\n",
            "Epoch: 01 | Epoch Time: 0m 1s\n",
            "Training Loss: 2063.955227322049 | Training Acc: 0.2935914099216461\n",
            "------------------------------------------------------------\n",
            "Epoch: 02 | Epoch Time: 0m 1s\n",
            "Training Loss: 1599.9321628146702 | Training Acc: 0.5311254262924194\n",
            "------------------------------------------------------------\n",
            "Epoch: 03 | Epoch Time: 0m 1s\n",
            "Training Loss: 1382.1099344889324 | Training Acc: 0.6621003150939941\n",
            "------------------------------------------------------------\n",
            "Epoch: 04 | Epoch Time: 0m 1s\n",
            "Training Loss: 1261.505852593316 | Training Acc: 0.7228924632072449\n",
            "------------------------------------------------------------\n",
            "Epoch: 05 | Epoch Time: 0m 1s\n",
            "Training Loss: 1179.469000922309 | Training Acc: 0.768978476524353\n",
            "------------------------------------------------------------\n",
            "Epoch: 06 | Epoch Time: 0m 1s\n",
            "Training Loss: 1118.6849297417534 | Training Acc: 0.8057814240455627\n",
            "------------------------------------------------------------\n",
            "Epoch: 07 | Epoch Time: 0m 1s\n",
            "Training Loss: 1079.8031616210938 | Training Acc: 0.8240788578987122\n",
            "------------------------------------------------------------\n",
            "Epoch: 08 | Epoch Time: 0m 1s\n",
            "Training Loss: 1051.2167256673176 | Training Acc: 0.8322724103927612\n",
            "------------------------------------------------------------\n",
            "Epoch: 09 | Epoch Time: 0m 1s\n",
            "Training Loss: 1030.5228135850693 | Training Acc: 0.8481577634811401\n",
            "------------------------------------------------------------\n",
            "Epoch: 10 | Epoch Time: 0m 1s\n",
            "Training Loss: 1013.6690538194445 | Training Acc: 0.8522401452064514\n",
            "------------------------------------------------------------\n",
            "Epoch: 11 | Epoch Time: 0m 1s\n",
            "Training Loss: 1001.1234876844618 | Training Acc: 0.8593512773513794\n",
            "------------------------------------------------------------\n",
            "Epoch: 12 | Epoch Time: 0m 1s\n",
            "Training Loss: 988.5728963216146 | Training Acc: 0.8707383871078491\n",
            "------------------------------------------------------------\n",
            "Epoch: 13 | Epoch Time: 0m 1s\n",
            "Training Loss: 978.3638712565104 | Training Acc: 0.8756021857261658\n",
            "------------------------------------------------------------\n",
            "Epoch: 14 | Epoch Time: 0m 1s\n",
            "Training Loss: 970.7408108181423 | Training Acc: 0.8795520067214966\n",
            "------------------------------------------------------------\n",
            "Epoch: 15 | Epoch Time: 0m 1s\n",
            "Training Loss: 963.2607082790798 | Training Acc: 0.8800179362297058\n",
            "------------------------------------------------------------\n",
            "Epoch: 16 | Epoch Time: 0m 1s\n",
            "Training Loss: 956.2274848090278 | Training Acc: 0.8862795829772949\n",
            "------------------------------------------------------------\n",
            "Epoch: 17 | Epoch Time: 0m 1s\n",
            "Training Loss: 949.2365993923611 | Training Acc: 0.892974853515625\n",
            "------------------------------------------------------------\n",
            "Epoch: 18 | Epoch Time: 0m 1s\n",
            "Training Loss: 942.0823771158854 | Training Acc: 0.8992831110954285\n",
            "------------------------------------------------------------\n",
            "Epoch: 19 | Epoch Time: 0m 1s\n",
            "Training Loss: 935.5263604058159 | Training Acc: 0.9020106792449951\n",
            "------------------------------------------------------------\n",
            "Epoch: 20 | Epoch Time: 0m 1s\n",
            "Training Loss: 930.5463663736979 | Training Acc: 0.9093727469444275\n",
            "------------------------------------------------------------\n",
            "Epoch: 21 | Epoch Time: 0m 1s\n",
            "Training Loss: 927.5273776584202 | Training Acc: 0.90453040599823\n",
            "------------------------------------------------------------\n",
            "Epoch: 22 | Epoch Time: 0m 1s\n",
            "Training Loss: 922.0774129231771 | Training Acc: 0.9142258167266846\n",
            "------------------------------------------------------------\n",
            "Epoch: 23 | Epoch Time: 0m 1s\n",
            "Training Loss: 917.9247300889757 | Training Acc: 0.9145339131355286\n",
            "------------------------------------------------------------\n",
            "Epoch: 24 | Epoch Time: 0m 1s\n",
            "Training Loss: 912.4146253797743 | Training Acc: 0.914419412612915\n",
            "------------------------------------------------------------\n",
            "Epoch: 25 | Epoch Time: 0m 1s\n",
            "Training Loss: 909.8370598687065 | Training Acc: 0.9168529510498047\n",
            "------------------------------------------------------------\n",
            "Epoch: 26 | Epoch Time: 0m 1s\n",
            "Training Loss: 906.9745619032118 | Training Acc: 0.9234480261802673\n",
            "------------------------------------------------------------\n",
            "Epoch: 27 | Epoch Time: 0m 1s\n",
            "Training Loss: 905.1994255913628 | Training Acc: 0.9225447773933411\n",
            "------------------------------------------------------------\n",
            "Epoch: 28 | Epoch Time: 0m 1s\n",
            "Training Loss: 898.3858439127604 | Training Acc: 0.9242150187492371\n",
            "------------------------------------------------------------\n",
            "Epoch: 29 | Epoch Time: 0m 1s\n",
            "Training Loss: 896.1620551215278 | Training Acc: 0.9258278608322144\n",
            "------------------------------------------------------------\n",
            "Epoch: 30 | Epoch Time: 0m 1s\n",
            "Training Loss: 893.8490329318577 | Training Acc: 0.929189920425415\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\"\"\"**Training using the soft labels**\"\"\"\n",
        "\n",
        "hotsize = 12\n",
        "assert len(train_softs[0]) == hotsize\n",
        "\n",
        "word_embedding_lookup = torch.from_numpy(word_emb).float().to(device)\n",
        "char_embedding_lookup = torch.from_numpy(char_emb).float().to(device)\n",
        "\n",
        "print('Beginning the Training')\n",
        "NUM_EXPERIMENTS = 1\n",
        "\n",
        "accs = [] #for training accuracy\n",
        "train_losses=[] # for training loss\n",
        "\n",
        "for exp in range(NUM_EXPERIMENTS):\n",
        "    print('\\nExperiment %d #######################'%exp)\n",
        "    last_batch = 0\n",
        "\n",
        "    model = RNN_all()\n",
        "    model = to_cuda(model)\n",
        "    optimizer = torch.optim.Adam(params=[p for p in model.parameters()],lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0\n",
        "        acc, dev_acc= 0 , 0\n",
        "        nepoch = epoch + 1\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        for word_pad_trn_bat, word_iis_trn_bat, char_pad_trn_bat, y_hot_trn_bat, y_soft_trn_bat in train_loader:\n",
        "            hard_predictions, hard_loss = model(word_pad_trn_bat, word_iis_trn_bat, char_pad_trn_bat, y_hot_trn_bat, y_soft_trn_bat, False)\n",
        "            backprop_hot(optimizer, hard_loss)\n",
        "            running_loss += hard_loss.item()            \n",
        "\n",
        "            # as Output of the network are log-probabilities, need to take exponential for probabilities\n",
        "            ps = torch.exp(hard_predictions)\n",
        "            top_p , top_class = ps.topk(1,dim=1)\n",
        "            equals = top_class == y_hot_trn_bat.view(*top_class.shape)\n",
        "\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc+= torch.mean(equals.type(torch.FloatTensor))\n",
        "            \n",
        "        #monitioring the epoch time            \n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "\n",
        "        #Traing Loss and Accuracy\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "        accs.append(acc/len(train_loader))\n",
        "        print(f'Training Loss: {train_losses[epoch]} | Training Acc: {accs[epoch].item()}')             \n",
        "\n",
        "        # evaluate after each epoch using \n",
        "        dev_hard_preds, dev_soft_preds = get_predictions(model, dev_loader)\n",
        "  \n",
        "        print('------------------------------------------------------------')\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Summary"
      ],
      "metadata": {
        "id": "qA03bLHKgFQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsNMRYLAM0bg",
        "outputId": "9348b7c8-e671-42f9-d092-a3eeba7d04ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN_all(\n",
            "  (word_encoder): Word_Encoder(\n",
            "    (gru): GRU(300, 100, batch_first=True, bidirectional=True)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (char_encoder): Char_Encoder(\n",
            "    (gru): GRU(64, 100, batch_first=True, bidirectional=True)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (char_attention): Attention(\n",
            "    (attn_nn): Sequential(\n",
            "      (0): Linear(in_features=200, out_features=400, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "  )\n",
            "  (word_attention): Attention(\n",
            "    (attn_nn): Sequential(\n",
            "      (0): Linear(in_features=200, out_features=400, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "  )\n",
            "  (fulcon): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=320, bias=True)\n",
            "    (1): Linear(in_features=320, out_features=256, bias=True)\n",
            "  )\n",
            "  (output_hot): Linear(in_features=256, out_features=12, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating model on Dev data**"
      ],
      "metadata": {
        "id": "on_Fy_NCgK-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"**Putting the data in the codalab answer format**\"\"\"\n",
        "#Gimpel-POS_answers.jsonlines- dev file\n",
        "\n",
        "codalab_dict = {str(i): {\"gold\": int(dev_hard_preds[i]), \"soft\": pred.tolist()} for i, pred in enumerate(dev_soft_preds)}\n",
        "codalab_dict = {\"dummy_name\": codalab_dict} \n",
        "\n",
        "with open(DATA_PATH +'/Gimpel-POS_answers-dev1.jsonlines', 'w') as f:\n",
        "    json.dump(codalab_dict, f)"
      ],
      "metadata": {
        "id": "sGBVDBdvMLGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Evaluating model on dev_data\"\"\"\n",
        "\n",
        "# Path of dev reference file\n",
        "dev_ref_path ='/content/drive/MyDrive/MscThesis/dev_reference_labels/Gimpel-POS_answers.jsonlines'\n",
        "# Path of dev result file\n",
        "dev_res_path= '/content/drive/MyDrive/MscThesis/public_data_PP/gimpel_pos/Gimpel-POS_answers-dev1.jsonlines'\n",
        "\n",
        "num_classes = 12\n",
        "f1, recall, precision = get_hard_score(dev_ref_path, dev_res_path, num_classes)\n",
        "soft_score_dev = get_soft_score(dev_ref_path, dev_res_path)\n",
        "print('Dev Baseline Model')\n",
        "print(f'Precision:{precision*100 : .2f}% | Recall: {recall*100: .2f}% ')\n",
        "print(f'F1 scores: {f1} | Cross-Entropy: {soft_score_dev}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OAX6Qc4Ph7x",
        "outputId": "09e6fd65-635e-497f-e94f-c6d97b50bd94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Baseline Model\n",
            "Precision: 77.41% | Recall:  77.01% \n",
            "F1 scores: 0.7573363584497147 | Cross-Entropy: 1.1122764255135051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating model on Test data**"
      ],
      "metadata": {
        "id": "scF5wt1CgMm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the test data from csv\n",
        "test_path = '/content/drive/MyDrive/MscThesis/public_data_evaluation/gimpel_pos/test_data.tsv'\n",
        "test_data = read_csv_data(test_path)\n",
        "\n",
        "# the indices of the words in a sentence, saved as arrays. Hint:Helps you know where each sentence ends\n",
        "test_iis_path = '/content/drive/MyDrive/MscThesis/public_data_evaluation/gimpel_pos/word_iis_tst.npy'\n",
        "word_iis_test = np.load(test_iis_path)\n",
        "\n",
        "# the padded sentences (maximum of 40 words per sentence). Two words in the same sentence will have the same word_pad. The numbers\n",
        "# indicate the idx of the word in the word embedding dictionary.\n",
        "test_wordpad_path = '/content/drive/MyDrive/MscThesis/public_data_evaluation/gimpel_pos/word_pad_tst.npy'\n",
        "word_pad_test = np.load(test_wordpad_path)\n",
        "\n",
        "# the character padding\n",
        "test_charpad_path = '/content/drive/MyDrive/MscThesis/public_data_evaluation/gimpel_pos/char_pad_tst.npy'\n",
        "char_pad_test = np.load(test_charpad_path)\n",
        "\n",
        "#\"\"\"**Getting, Tensorizing and Batching the Data**\"\"\"\n",
        "word_pad_tst_tens, word_iis_tst_tens, char_pad_tst_tens, hot_tst_tens, soft_tst_tens = create_dataset(word_pad_test, word_iis_test, char_pad_test)\n",
        "test = data_utils.TensorDataset(word_pad_tst_tens, word_iis_tst_tens, char_pad_tst_tens)\n",
        "test_loader = data_utils.DataLoader(test, batch_size=batsize, shuffle=False)\n"
      ],
      "metadata": {
        "id": "-QT1PnBwnj4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_hard_preds, test_soft_preds = get_predictions(model, test_loader)"
      ],
      "metadata": {
        "id": "feVifWjWYS5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BJshomSKXyn"
      },
      "outputs": [],
      "source": [
        "\"\"\"**Putting the data in the codalab answer format**\"\"\"\n",
        "#Gimpel-POS_answers.jsonlines- test file\n",
        "\n",
        "codalab_dict = {str(i): {\"gold\": int(test_hard_preds[i]), \"soft\": pred.tolist()} for i, pred in enumerate(test_soft_preds)}\n",
        "codalab_dict = {\"dummy_name\": codalab_dict} \n",
        "with open(DATA_PATH +'/Gimpel-POS_answers-test1.jsonlines', 'w') as f:\n",
        "    json.dump(codalab_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of test reference file\n",
        "test_ref_path= '/content/drive/MyDrive/MscThesis/test_reference_data/Gimpel-POS_answers.jsonlines'\n",
        "# Path of test result file\n",
        "test_res_path= '/content/drive/MyDrive/MscThesis/public_data_PP/gimpel_pos/Gimpel-POS_answers-test1.jsonlines'\n",
        "\n",
        "num_classes = 12\n",
        "f1, recall, precision  = get_hard_score(test_ref_path, test_res_path, num_classes)\n",
        "soft_score = get_soft_score(test_ref_path, test_res_path)\n",
        "\n",
        "print('Test Baseline Model')\n",
        "print(f'Precision:{precision*100 : .2f}% | Recall: {recall*100: .2f}% ')\n",
        "print(f'F1 scores: {f1} | Cross-Entropy: {soft_score_dev}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-mYpJuDPdsM",
        "outputId": "968c9710-2b98-49f0-9edb-fd8b7ddb6cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Baseline Model\n",
            "Precision: 77.75% | Recall:  77.08% \n",
            "F1 scores: 0.7610792178139648 | Cross-Entropy: 1.1122764255135051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**References**"
      ],
      "metadata": {
        "id": "FVTuM89Ferja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Pytorch.org. 2022. PyTorch documentation — PyTorch 1.12 documentation. [online] Available at: <https://pytorch.org/docs/stable/index.html> [Accessed 21 August 2022].\n",
        "\n",
        "*  Numpy.org. 2022. NumPy documentation — NumPy v1.23 Manual. [online] Available at: <https://numpy.org/doc/stable/> [Accessed 21 August 2022].\n",
        "\n",
        "*   Spacy.io. 2022. spaCy- Documentation. [online] Available at: <https://spacy.io/api> [Accessed 21 August 2022].\n",
        "\n",
        "*   Pytorch.org. 2022. GRU — PyTorch 1.12 documentation. [online] Available at: <https://pytorch.org/docs/stable/generated/torch.nn.GRU.html> [Accessed 21 August 2022].\n",
        "\n",
        "*  Pytorch.org. 2022. RNN — PyTorch 1.12 documentation. [online] Available at: <https://pytorch.org/docs/stable/generated/torch.nn.RNN.html> [Accessed 21 August 2022].\n",
        "\n",
        "*  Stack Overflow. 2022. Stack Overflow - Where Developers Learn, Share, & Build Careers. [online] Available at: <https://stackoverflow.com/> [Accessed 21 August 2022].\n"
      ],
      "metadata": {
        "id": "dkNkOHvAerSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8iF-oxnzeq_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "POS_GRU_ReLU.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}